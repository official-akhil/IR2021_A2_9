{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'d:\\\\Data\\\\IIIT_Delhi\\\\2_Semester\\\\1_IR\\\\Assignments\\\\A2'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\shrad\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#Import header files\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from num2words import num2words\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import numpy as np \n",
    "# np.set_printoptions(threshold = np.inf)\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Folder Locations =  ['d:\\\\Data\\\\IIIT_Delhi\\\\2_Semester\\\\1_IR\\\\Assignments\\\\A2/stories', 'd:\\\\Data\\\\IIIT_Delhi\\\\2_Semester\\\\1_IR\\\\Assignments\\\\A2/stories/FARNON', 'd:\\\\Data\\\\IIIT_Delhi\\\\2_Semester\\\\1_IR\\\\Assignments\\\\A2/stories/SRE']\n"
     ]
    }
   ],
   "source": [
    "#Retrieve all folders\n",
    "locations = [pos[0] for pos in os.walk(str(os.getcwd())+'/'+'stories'+'/')]\n",
    "len_locations = len(locations[0])\n",
    "locations[0] = locations[0][:len_locations-1]\n",
    "\n",
    "print(\"Folder Locations = \",locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of files = 467\n"
     ]
    }
   ],
   "source": [
    "#Load all files from index file\n",
    "dataset = []\n",
    "\n",
    "flag = False\n",
    "\n",
    "for i in locations:\n",
    "    file = open(i + \"/index.html\", 'r')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "\n",
    "    file_name = re.findall('><A HREF=\"(.*)\">', text)\n",
    "    file_title = re.findall('<BR><TD> (.*)\\n', text)\n",
    "\n",
    "    if flag == False:\n",
    "        file_name = file_name[2:]\n",
    "        flag = True\n",
    "\n",
    "    for j in range(len(file_name)):\n",
    "        dataset.append((str(i) + \"/\" + str(file_name[j]), file_title[j]))\n",
    "\n",
    "N = len (dataset)\n",
    "print(\"Total number of files =\",N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing functions\n",
    "def l_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "def punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;,<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    return data\n",
    "\n",
    "def apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stop_words(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_word = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_word = new_word + \" \" + w\n",
    "    return np.char.strip(new_word)\n",
    "\n",
    "def lemming(data):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_word = \"\"\n",
    "    for w in tokens:\n",
    "        new_word = new_word + \" \" + wordnet_lemmatizer.lemmatize(w)\n",
    "    return np.char.strip(new_word)\n",
    "\n",
    "def stem_text(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def process_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = l_case(text)\n",
    "    text = punctuation(text)\n",
    "    text = apostrophe(text)\n",
    "    text = stop_words(text)\n",
    "    # text = process_numbers(text)\n",
    "    # text = lemming(text)\n",
    "    # text = stem_text(text)\n",
    "    # text = punctuation(text)\n",
    "    # text = process_numbers(text)\n",
    "    # text = stem_text(text) \n",
    "    # text = punctuation(text) \n",
    "    # text = stop_words(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = []\n",
    "processed_title = []\n",
    "\n",
    "for i in dataset[:N]:\n",
    "    file = open(i[0], 'r', encoding=\"utf8\", errors='ignore')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "\n",
    "    processed_text.append(word_tokenize(str(preprocess(text))))\n",
    "    processed_title.append(word_tokenize(str(preprocess(i[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = {}\n",
    "\n",
    "for i in range(N):\n",
    "    tokens = processed_text[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "\n",
    "    tokens = processed_title[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# len(processed_text[466])\n",
    "len(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_size = len(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "52385"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "total_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab = [x for x in DF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['shareware', 'trial', 'project', 'freeware', 'need', 'support', 'continue', '100', 'west', '53', 'north', 'jim', 'prentice', 'copyright', '1990', 'brandon', 'manitoba', 'canada', 'magic', 'phrase']\n"
     ]
    }
   ],
   "source": [
    "print(total_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-f07833d527fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mtf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mtf_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[1;31m# df = doc_freq(token)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;31m# idf = np.log((N+1)/(df+1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Term Frequency\n",
    "# doc = 0\n",
    "TF_idf = np.zeros((N, len(total_vocab)))\n",
    "\n",
    "# tf_idf = {}\n",
    "tf_idf1 =[]\n",
    "\n",
    "for i in range(N):\n",
    "    tf_vec = []\n",
    "    tf_vec1 =[]\n",
    "    tokens = processed_text[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_title[i])\n",
    "    words_count = len(tokens + processed_title[i])\n",
    "    \n",
    "    # for token in np.unique(tokens):\n",
    "        \n",
    "    #     tf = counter[token]/words_count\n",
    "    #     df = doc_freq(token)\n",
    "    #     idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "    #     tf_idf[doc, token] = tf*idf\n",
    "\n",
    "    for token in total_vocab:\n",
    "        if token in processed_text[i]:\n",
    "           \n",
    "            tf = counter[token]/words_count\n",
    "            tf_vec.append(tf)\n",
    "            # df = doc_freq(token)\n",
    "            # idf = np.log((N+1)/(df+1))\n",
    "            # tf_idf[doc, token] = tf*idf\n",
    "           \n",
    "        else:\n",
    "            \n",
    "            tf = 0\n",
    "            tf_vec.append(tf)\n",
    "            # df = doc_freq(token)\n",
    "            # idf = np.log((N+1)/(df+1))\n",
    "            # tf_idf[doc, token] = tf*idf\n",
    "\n",
    "    for j in range(len(total_vocab)):\n",
    "\n",
    "         df = doc_freq(total_vocab[j])\n",
    "         idf = np.log((N+1)/(df+1))\n",
    "\n",
    "    #     # if(total_vocab[j] in processed_text[i][j]  ):\n",
    "         temp = tf_vec[j]*idf\n",
    "         tf_vec1.append(temp)\n",
    "   \n",
    "         TF_idf[i][j] = tf_vec[j]*idf\n",
    "    tf_idf1.append(tf_vec1)\n",
    "\n",
    "\n",
    " \n",
    "    # doc += 1\n",
    "\n",
    "np.save( \"TFidftermfrequency\",TF_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matching Score\n",
      "\n",
      "Query: THE CRAB AND THE HERON\n",
      "\n",
      "['crab', 'heron']\n",
      "\n",
      "[122, 26, 270, 418, 169]\n",
      "0.41362530209306514\n",
      "0.0016366746585185015\n",
      "0.001309303929884175\n",
      "0.0008390118423461789\n",
      "0.000632598929387192\n",
      "['crabhern.txt', 'aesop11.txt', 'long1-3.txt', 'timem.hac', 'fgoose.txt'] \n",
      "\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018439642087588456, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7973139054443426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006676890533805379, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010776157800016617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011905221345725024, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.006676890533805379, 0.010776157800016617, 0.011905221345725024, 0.018439642087588456, 0.7973139054443426]\n",
      "0.7973139054443426  and index is  122\n",
      "0.018439642087588456  and index is  26\n",
      "0.011905221345725024  and index is  418\n",
      "0.010776157800016617  and index is  270\n",
      "0.006676890533805379  and index is  169\n",
      "\n",
      "List of Top Documents wrt Cosine Similarity for i/p query is:\n",
      "crabhern.txt  with Cosine Similarity Value: 0.7973139054443426 \n",
      " aesop11.txt  with Cosine Similarity Value: 0.018439642087588456 \n",
      " timem.hac  with Cosine Similarity Value: 0.011905221345725024 \n",
      " long1-3.txt  with Cosine Similarity Value: 0.010776157800016617 \n",
      " fgoose.txt  with Cosine Similarity Value: 0.006676890533805379 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def matching_score_2(k, query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    q_vec =[]\n",
    "    tfscore = 0\n",
    "    alltfscore =[]\n",
    "\n",
    "    for i in range(len(total_vocab)):\n",
    "        if total_vocab[i] in tokens:\n",
    "            Q[i] = 1\n",
    "        else:\n",
    "            Q[i] = 0\n",
    "    # print(Q)\n",
    "\n",
    "    for token in total_vocab:\n",
    "        if token in tokens:\n",
    "            q_vec.append(1)\n",
    "        else:\n",
    "            q_vec.append(0)\n",
    "    # print(q_vec)\n",
    "\n",
    "    for j in range(N):\n",
    "        tfscore = 0\n",
    "        for k in range(len(total_vocab)):\n",
    "            if(q_vec[k] == 1):\n",
    "                tfscore += TF_idf[j][k]\n",
    "            else:\n",
    "                tfscore = tfscore\n",
    "        alltfscore.append(tfscore)\n",
    "    # print(alltfscore)\n",
    "\n",
    "    alltfscore_sorted = sorted(alltfscore, reverse=True)\n",
    "    # print(alltfscore_sorted)\n",
    "\n",
    "    # print(sorted(range(len(alltfscore)), key=lambda i: alltfscore[i], reverse=True)[:5])\n",
    "    print(\"\")\n",
    "    docName = []\n",
    "\n",
    "    li=[]\n",
    "    for i in range(len(alltfscore)):\n",
    "      li.append([alltfscore[i],i])\n",
    "    li.sort( reverse=True)\n",
    "    sort_index = []\n",
    "    \n",
    "    for x in li:\n",
    "        sort_index.append(x[1])\n",
    "    \n",
    "    print((sort_index)[:5])\n",
    "    for index in((sort_index)[:5]):\n",
    "        head, tail = os.path.split(dataset[index][0])\n",
    "        docName.append(tail)\n",
    "        print(alltfscore[index])\n",
    "    print(docName,\"\\n\")\n",
    "    # 11, 129, 224, 236, 366\n",
    "    # print(alltfscore[11], alltfscore[129], alltfscore[224], alltfscore[236], alltfscore[366]) \n",
    "    \n",
    "    # Document Vectorization\n",
    "    cosine_score_term_freq = []\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    D = np.zeros((N, total_vocab_size))\n",
    "    q_vec_1 = np.asarray(q_vec, dtype=np.float32)\n",
    "    for i in TF_idf:       \n",
    "        doc = i.reshape(1,total_vocab_size)\n",
    "        doc = np.array(doc)\n",
    "        # print(type(doc))\n",
    "        query = q_vec_1.reshape(1,total_vocab_size)\n",
    "        query = np.array(query)\n",
    "        cosSim = cosine_similarity(doc, query)\n",
    "        # list1 = cosSim[0].tolist()\n",
    "        list1 = cosSim[0].item()\n",
    "        cosine_score_term_freq.append(list1)\n",
    "    print(cosine_score_term_freq)\n",
    "\n",
    "    cosine_score_term_freq_sorted = sorted(cosine_score_term_freq)\n",
    "    docName = []\n",
    "    top5 = cosine_score_term_freq_sorted[-5:]\n",
    "    print(top5)\n",
    "    for i in range(5):        \n",
    "        print(top5[4-i],\" and index is \",cosine_score_term_freq.index(top5[4-i]))\n",
    "        index = cosine_score_term_freq.index(top5[4-i])\n",
    "        head, tail = os.path.split(dataset[index][0])\n",
    "        tail.strip()\n",
    "        docName.append(tail.strip())\n",
    "        docName.append(\" with Cosine Similarity Value:\")\n",
    "        docName.append(top5[4-i])\n",
    "        docName.append(\"\\n\")\n",
    "    print(\"\\nList of Top Documents wrt Cosine Similarity for i/p query is:\")\n",
    "    print(*docName)\n",
    "    \n",
    "matching_score_2(5, \"THE CRAB AND THE HERON\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary tf-idf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "TF_idf0 =  np.zeros((N, len(total_vocab)))\n",
    "tf_idf0 = []\n",
    "Bin_tf = []\n",
    "bin_tfidf_df = pd.DataFrame()\n",
    "for p in range(N):\n",
    "\n",
    "    sent_vec = []\n",
    "    tf_vec0 =[]\n",
    "\n",
    "    for token in total_vocab:\n",
    "        if token in processed_text[p]:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)   \n",
    "\n",
    "    \n",
    "\n",
    "    for j in range(len(total_vocab)):\n",
    "\n",
    "        df0 = doc_freq(total_vocab[j])\n",
    "        idf0 = np.log((N+1)/(df0+1))\n",
    "\n",
    "        temp = sent_vec[j]*idf0\n",
    "        tf_vec0.append(temp)\n",
    "       \n",
    "        TF_idf0[p][j]= sent_vec[j]*idf0\n",
    "\n",
    "    Bin_tf.append(sent_vec)\n",
    "    tf_idf0.append(tf_vec0)\n",
    "    # bin_tfidf_df.append(tf_vec0)\n",
    "    # bin_tfidf_df = bin_tfidf_df.append(tf_vec0,ignore_index=True)\n",
    "    bin_tfidf_df[i] = tf_vec0\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save( \"TFidfBinary\",TF_idf0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matching Score\n",
      "\n",
      "Query: THE CRAB AND THE HERON\n",
      "\n",
      "['crab', 'heron']\n",
      "\n",
      "[122, 26, 418, 270, 169]\n",
      "9.406564833939129\n",
      "9.406564833939129\n",
      "4.356708826689592\n",
      "4.356708826689592\n",
      "4.356708826689592\n",
      "['crabhern.txt', 'aesop11.txt', 'timem.hac', 'long1-3.txt', 'fgoose.txt']\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.025439293354229037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2401234519815826, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01770831819721052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0211669121083841, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013841932742447044, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.013841932742447044, 0.01770831819721052, 0.0211669121083841, 0.025439293354229037, 0.2401234519815826]\n",
      "0.2401234519815826  and index is  122\n",
      "0.025439293354229037  and index is  26\n",
      "0.0211669121083841  and index is  270\n",
      "0.01770831819721052  and index is  169\n",
      "0.013841932742447044  and index is  418\n",
      "\n",
      "List of Top Documents wrt Cosine Similarity for i/p query(Binary TF Weighting) is:\n",
      "crabhern.txt  with Cosine Similarity Value: 0.2401234519815826 \n",
      " aesop11.txt  with Cosine Similarity Value: 0.025439293354229037 \n",
      " long1-3.txt  with Cosine Similarity Value: 0.0211669121083841 \n",
      " fgoose.txt  with Cosine Similarity Value: 0.01770831819721052 \n",
      " timem.hac  with Cosine Similarity Value: 0.013841932742447044 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def matching_score(k, query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    q_vec =[]\n",
    "    tfscore = 0\n",
    "    alltfscore =[]\n",
    "\n",
    "    for i in range(len(total_vocab)):\n",
    "        if total_vocab[i] in tokens:\n",
    "            Q[i] = 1\n",
    "        else:\n",
    "            Q[i] = 0\n",
    "    # print(Q)\n",
    "\n",
    "    for token in total_vocab:\n",
    "        if token in tokens:\n",
    "            q_vec.append(1)\n",
    "        else:\n",
    "            q_vec.append(0)\n",
    "    # print(q_vec)\n",
    "\n",
    "    for j in range(N):\n",
    "        tfscore = 0\n",
    "        for k in range(len(total_vocab)):\n",
    "            if(q_vec[k] == 1):\n",
    "                tfscore += TF_idf0[j][k]\n",
    "            else:\n",
    "                tfscore = tfscore\n",
    "        alltfscore.append(tfscore)\n",
    "    # print(alltfscore)\n",
    "\n",
    "    alltfscore_sorted = sorted(alltfscore, reverse=True)\n",
    "    # print(alltfscore_sorted)\n",
    "\n",
    "    # print(sorted(range(len(alltfscore)), key=lambda i: alltfscore[i], reverse=True)[:5])\n",
    "    print(\"\")\n",
    "    docName = []\n",
    "\n",
    "    li=[]\n",
    "    for i in range(len(alltfscore)):\n",
    "      li.append([alltfscore[i],i])\n",
    "    li.sort( reverse=True)\n",
    "    sort_index = []\n",
    "    \n",
    "    for x in li:\n",
    "        sort_index.append(x[1])\n",
    "    \n",
    "    print((sort_index)[:5])\n",
    "    for index in((sort_index)[:5]):\n",
    "        head, tail = os.path.split(dataset[index][0])\n",
    "        docName.append(tail)\n",
    "        print(alltfscore[index])\n",
    "    print(docName)\n",
    "    # 11, 129, 224, 236, 366\n",
    "    # print(alltfscore[11], alltfscore[129], alltfscore[224], alltfscore[236], alltfscore[366])  \n",
    "\n",
    "    # Document Vectorization\n",
    "    cosine_score_binary = []\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    D = np.zeros((N, total_vocab_size))\n",
    "    q_vec_1 = np.asarray(q_vec, dtype=np.float32)\n",
    "    for i in TF_idf0:       \n",
    "        doc = i.reshape(1,total_vocab_size)\n",
    "        doc = np.array(doc)\n",
    "        # print(type(doc))\n",
    "        query = q_vec_1.reshape(1,total_vocab_size)\n",
    "        query = np.array(query)\n",
    "        cosSim = cosine_similarity(doc, query)\n",
    "        # list1 = cosSim[0].tolist()\n",
    "        list1 = cosSim[0].item()\n",
    "        cosine_score_binary.append(list1)\n",
    "    print(cosine_score_binary)\n",
    "\n",
    "    cosine_score_binary_sorted = sorted(cosine_score_binary)\n",
    "    docName = []\n",
    "    top5 = cosine_score_binary_sorted[-5:]\n",
    "    print(top5)\n",
    "    for i in range(5):        \n",
    "        print(top5[4-i],\" and index is \",cosine_score_binary.index(top5[4-i]))\n",
    "        index = cosine_score_binary.index(top5[4-i])\n",
    "        head, tail = os.path.split(dataset[index][0])\n",
    "        tail.strip()\n",
    "        docName.append(tail.strip())\n",
    "        docName.append(\" with Cosine Similarity Value:\")\n",
    "        docName.append(top5[4-i])\n",
    "        docName.append(\"\\n\")\n",
    "    print(\"\\nList of Top Documents wrt Cosine Similarity for i/p query(Binary TF Weighting) is:\")\n",
    "    print(*docName)  \n",
    "    \n",
    "matching_score(5, \"THE CRAB AND THE HERON\")    \n",
    "# matching_score(5, \"I will endeavour, in my statement, to avoid such terms as would serve to limit the events to any particular place, or give a clue as to the people concerned.\")\n",
    "# matching_score(5, \"Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF RAW COUNT\n",
    "# doc = 0\n",
    "TF_idf_raw = np.zeros((N, len(total_vocab)))\n",
    "\n",
    "# tf_idf_r = {}\n",
    "tf_idf1_r =[]\n",
    "\n",
    "for i in range(N):\n",
    "    tf_vec_r = []\n",
    "    tf_vec1_r =[]\n",
    "    tokens = processed_text[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_title[i])\n",
    "    words_count = len(tokens + processed_title[i])\n",
    "    \n",
    "    \n",
    "\n",
    "    for token in total_vocab:\n",
    "        if token in processed_text[i]:\n",
    "\n",
    "            tf = counter[token]\n",
    "            # tf = counter[token]/words_count\n",
    "            tf_vec_r.append(tf)\n",
    "          \n",
    "           \n",
    "        else:\n",
    "            \n",
    "            tf = 0\n",
    "            tf_vec_r.append(tf)\n",
    "          \n",
    "\n",
    "    for j in range(len(total_vocab)):\n",
    "\n",
    "         df = doc_freq(total_vocab[j])\n",
    "         idf = np.log((N+1)/(df+1))\n",
    "\n",
    "    \n",
    "         temp = tf_vec_r[j]*idf\n",
    "         tf_vec1_r.append(temp)\n",
    "   \n",
    "         TF_idf_raw[i][j] = tf_vec_r[j]*idf\n",
    "    tf_idf1_r.append(tf_vec1_r)\n",
    "\n",
    "\n",
    " \n",
    "    # doc += 1\n",
    "\n",
    "np.save( \"TFidfrawcount\",TF_idf_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matching Score\n",
      "\n",
      "Query: THE CRAB AND THE HERON\n",
      "\n",
      "['crab', 'heron']\n",
      "\n",
      "[122, 26, 418, 270, 169]\n",
      "83.96593632489221\n",
      "31.19010896738709\n",
      "13.070126480068776\n",
      "8.713417653379183\n",
      "4.356708826689592\n",
      "['crabhern.txt', 'aesop11.txt', 'timem.hac', 'long1-3.txt', 'fgoose.txt']\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018439642087588422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7973139054443423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006676890533805383, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010776157800016615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011905221345725029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.006676890533805383, 0.010776157800016615, 0.011905221345725029, 0.018439642087588422, 0.7973139054443423]\n",
      "0.7973139054443423  and index is  122\n",
      "0.018439642087588422  and index is  26\n",
      "0.011905221345725029  and index is  418\n",
      "0.010776157800016615  and index is  270\n",
      "0.006676890533805383  and index is  169\n",
      "\n",
      "List of Top Documents wrt Cosine Similarity for i/p query(Raw count) is:\n",
      "crabhern.txt  with Cosine Similarity Value: 0.7973139054443423 \n",
      " aesop11.txt  with Cosine Similarity Value: 0.018439642087588422 \n",
      " timem.hac  with Cosine Similarity Value: 0.011905221345725029 \n",
      " long1-3.txt  with Cosine Similarity Value: 0.010776157800016615 \n",
      " fgoose.txt  with Cosine Similarity Value: 0.006676890533805383 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def matching_score_3(k, query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    q_vec =[]\n",
    "    tfscore = 0\n",
    "    alltfscore =[]\n",
    "\n",
    "    for i in range(len(total_vocab)):\n",
    "        if total_vocab[i] in tokens:\n",
    "            Q[i] = 1\n",
    "        else:\n",
    "            Q[i] = 0\n",
    "    # print(Q)\n",
    "\n",
    "    for token in total_vocab:\n",
    "        if token in tokens:\n",
    "            q_vec.append(1)\n",
    "        else:\n",
    "            q_vec.append(0)\n",
    "    # print(q_vec)\n",
    "\n",
    "    for j in range(N):\n",
    "        tfscore = 0\n",
    "        for k in range(len(total_vocab)):\n",
    "            if(q_vec[k] == 1):\n",
    "                tfscore += TF_idf_raw[j][k]\n",
    "            else:\n",
    "                tfscore = tfscore\n",
    "        alltfscore.append(tfscore)\n",
    "    # print(alltfscore)\n",
    "\n",
    "    alltfscore_sorted = sorted(alltfscore, reverse=True)\n",
    "    # print(alltfscore_sorted)\n",
    "\n",
    "    # print(sorted(range(len(alltfscore)), key=lambda i: alltfscore[i], reverse=True)[:5])\n",
    "    print(\"\")\n",
    "    docName = []\n",
    "\n",
    "    li=[]\n",
    "    for i in range(len(alltfscore)):\n",
    "      li.append([alltfscore[i],i])\n",
    "    li.sort( reverse=True)\n",
    "    sort_index = []\n",
    "    \n",
    "    for x in li:\n",
    "        sort_index.append(x[1])\n",
    "    \n",
    "    print((sort_index)[:5])\n",
    "    for index in((sort_index)[:5]):\n",
    "        head, tail = os.path.split(dataset[index][0])\n",
    "        docName.append(tail)\n",
    "        print(alltfscore[index])\n",
    "    print(docName)\n",
    "    # 11, 129, 224, 236, 366\n",
    "    # print(alltfscore[11], alltfscore[129], alltfscore[224], alltfscore[236], alltfscore[366])    \n",
    "     # Document Vectorization\n",
    "    cosine_score_raw = []\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    D = np.zeros((N, total_vocab_size))\n",
    "    q_vec_1 = np.asarray(q_vec, dtype=np.float32)\n",
    "    for i in TF_idf_raw:       \n",
    "        doc = i.reshape(1,total_vocab_size)\n",
    "        doc = np.array(doc)\n",
    "        # print(type(doc))\n",
    "        query = q_vec_1.reshape(1,total_vocab_size)\n",
    "        query = np.array(query)\n",
    "        cosSim = cosine_similarity(doc, query)\n",
    "        # list1 = cosSim[0].tolist()\n",
    "        list1 = cosSim[0].item()\n",
    "        cosine_score_raw.append(list1)\n",
    "    print(cosine_score_raw)\n",
    "\n",
    "    cosine_score_raw_sorted = sorted(cosine_score_raw)\n",
    "    docName = []\n",
    "    top5 = cosine_score_raw_sorted[-5:]\n",
    "    print(top5)\n",
    "    for i in range(5):        \n",
    "        print(top5[4-i],\" and index is \",cosine_score_raw.index(top5[4-i]))\n",
    "        index = cosine_score_raw.index(top5[4-i])\n",
    "        head, tail = os.path.split(dataset[index][0])\n",
    "        tail.strip()\n",
    "        docName.append(tail.strip())\n",
    "        docName.append(\" with Cosine Similarity Value:\")\n",
    "        docName.append(top5[4-i])\n",
    "        docName.append(\"\\n\")\n",
    "    print(\"\\nList of Top Documents wrt Cosine Similarity for i/p query(Raw count) is:\")\n",
    "    print(*docName)  \n",
    "    \n",
    "matching_score_3(5, \"THE CRAB AND THE HERON\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log normalisation \n",
    "\n",
    "# doc = 0\n",
    "TF_idf_logn = np.zeros((N, len(total_vocab)))\n",
    "\n",
    "# tf_idf_ln = {}\n",
    "tf_idf1_ln =[]\n",
    "\n",
    "for i in range(N):\n",
    "    tf_vec = []\n",
    "    tf_vec1 =[]\n",
    "    tokens = processed_text[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_title[i])\n",
    "    words_count = len(tokens + processed_title[i])\n",
    "    \n",
    "\n",
    "    for token in total_vocab:\n",
    "        if token in processed_text[i]:\n",
    "           \n",
    "            # tf = counter[token]/words_count\n",
    "            tf = np.log(1 + counter[token])\n",
    "            tf_vec.append(tf)\n",
    "          \n",
    "        else:\n",
    "            \n",
    "            tf = 0\n",
    "            tf_vec.append(tf)\n",
    "           \n",
    "\n",
    "    for j in range(len(total_vocab)):\n",
    "\n",
    "         df = doc_freq(total_vocab[j])\n",
    "         idf = np.log((N+1)/(df+1))\n",
    "\n",
    "    \n",
    "         temp = tf_vec[j]*idf\n",
    "         tf_vec1.append(temp)\n",
    "   \n",
    "         TF_idf_logn[i][j] = tf_vec[j]*idf\n",
    "    tf_idf1_ln.append(tf_vec1)\n",
    "\n",
    "\n",
    " \n",
    "    # doc += 1\n",
    "\n",
    "np.save( \"TFidflogNormalisation\",TF_idf_logn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matching Score\n",
      "\n",
      "Query: THE CRAB AND THE HERON\n",
      "\n",
      "['crab', 'heron']\n",
      "\n",
      "[122, 26, 418, 270, 169]\n",
      "21.54259923161554\n",
      "11.978057375992861\n",
      "6.039680879481035\n",
      "4.786333855150008\n",
      "3.0198404397405176\n",
      "['crabhern.txt', 'aesop11.txt', 'timem.hac', 'long1-3.txt', 'fgoose.txt']\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03016404874411095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5607367318281735, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.012887410111712457, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.023900241380333767, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019228825736402343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.012887410111712457, 0.019228825736402343, 0.023900241380333767, 0.03016404874411095, 0.5607367318281735]\n",
      "0.5607367318281735  and index is  122\n",
      "0.03016404874411095  and index is  26\n",
      "0.023900241380333767  and index is  270\n",
      "0.019228825736402343  and index is  418\n",
      "0.012887410111712457  and index is  169\n",
      "\n",
      "List of Top Documents wrt Cosine Similarity for i/p query(Log Normalization) is:\n",
      "crabhern.txt  with Cosine Similarity Value: 0.5607367318281735 \n",
      " aesop11.txt  with Cosine Similarity Value: 0.03016404874411095 \n",
      " long1-3.txt  with Cosine Similarity Value: 0.023900241380333767 \n",
      " timem.hac  with Cosine Similarity Value: 0.019228825736402343 \n",
      " fgoose.txt  with Cosine Similarity Value: 0.012887410111712457 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def matching_score_4(k, query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    q_vec =[]\n",
    "    tfscore = 0\n",
    "    alltfscore =[]\n",
    "\n",
    "    for i in range(len(total_vocab)):\n",
    "        if total_vocab[i] in tokens:\n",
    "            Q[i] = 1\n",
    "        else:\n",
    "            Q[i] = 0\n",
    "    # print(Q)\n",
    "\n",
    "    for token in total_vocab:\n",
    "        if token in tokens:\n",
    "            q_vec.append(1)\n",
    "        else:\n",
    "            q_vec.append(0)\n",
    "    # print(q_vec)\n",
    "\n",
    "    for j in range(N):\n",
    "        tfscore = 0\n",
    "        for k in range(len(total_vocab)):\n",
    "            if(q_vec[k] == 1):\n",
    "                tfscore += TF_idf_logn[j][k]\n",
    "            else:\n",
    "                tfscore = tfscore\n",
    "        alltfscore.append(tfscore)\n",
    "    # print(alltfscore)\n",
    "\n",
    "    alltfscore_sorted = sorted(alltfscore, reverse=True)\n",
    "    # print(alltfscore_sorted)\n",
    "\n",
    "    # print(sorted(range(len(alltfscore)), key=lambda i: alltfscore[i], reverse=True)[:5])\n",
    "    print(\"\")\n",
    "    docName = []\n",
    "\n",
    "    li=[]\n",
    "    for i in range(len(alltfscore)):\n",
    "      li.append([alltfscore[i],i])\n",
    "    li.sort( reverse=True)\n",
    "    sort_index = []\n",
    "    \n",
    "    for x in li:\n",
    "        sort_index.append(x[1])\n",
    "    \n",
    "    print((sort_index)[:5])\n",
    "    for index in((sort_index)[:5]):\n",
    "        head, tail = os.path.split(dataset[index][0])\n",
    "        docName.append(tail)\n",
    "        print(alltfscore[index])\n",
    "    print(docName)\n",
    "    # 11, 129, 224, 236, 366\n",
    "    # print(alltfscore[11], alltfscore[129], alltfscore[224], alltfscore[236], alltfscore[366])    \n",
    "\n",
    "    # Document Vectorization\n",
    "    cosine_score_log_nrml = []\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    D = np.zeros((N, total_vocab_size))\n",
    "    q_vec_1 = np.asarray(q_vec, dtype=np.float32)\n",
    "    for i in TF_idf_logn:       \n",
    "        doc = i.reshape(1,total_vocab_size)\n",
    "        doc = np.array(doc)\n",
    "        # print(type(doc))\n",
    "        query = q_vec_1.reshape(1,total_vocab_size)\n",
    "        query = np.array(query)\n",
    "        cosSim = cosine_similarity(doc, query)\n",
    "        # list1 = cosSim[0].tolist()\n",
    "        list1 = cosSim[0].item()\n",
    "        cosine_score_log_nrml.append(list1)\n",
    "    print(cosine_score_log_nrml)\n",
    "\n",
    "    cosine_score_log_nrml_sorted = sorted(cosine_score_raw)\n",
    "    docName = []\n",
    "    top5 = cosine_score_log_nrml_sorted[-5:]\n",
    "    print(top5)\n",
    "    for i in range(5):        \n",
    "        print(top5[4-i],\" and index is \",cosine_score_log_nrml.index(top5[4-i]))\n",
    "        index = cosine_score_log_nrml.index(top5[4-i])\n",
    "        head, tail = os.path.split(dataset[index][0])\n",
    "        tail.strip()\n",
    "        docName.append(tail.strip())\n",
    "        docName.append(\" with Cosine Similarity Value:\")\n",
    "        docName.append(top5[4-i])\n",
    "        docName.append(\"\\n\")\n",
    "    print(\"\\nList of Top Documents wrt Cosine Similarity for i/p query(Log Normalization) is:\")\n",
    "    print(*docName)  \n",
    "# matching_score_5(5, \"THE CRAB AND THE HERON\")\n",
    "    \n",
    "matching_score_4(5, \"THE CRAB AND THE HERON\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double  Normalisation \n",
    "\n",
    "TF_idf_dbl = np.zeros((N, len(total_vocab)))\n",
    "\n",
    "# tf_idf = {}\n",
    "tf_idf1_dbl =[]\n",
    "max_count = 0\n",
    "\n",
    "# for i in range(N):\n",
    "#     all_word_count =[]\n",
    "#     tokens = processed_text[i]\n",
    "#     words_count = len(tokens + processed_title[i])\n",
    "#     all_word_count.append(words_count)\n",
    "# max_count = max(all_word_count)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(N):\n",
    "    tf_vec = []\n",
    "    tf_vec1 =[]\n",
    "    tokens = processed_text[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_title[i])\n",
    "    # words_count = len(tokens + processed_title[i])\n",
    "    \n",
    "    count_max_freq = counter.most_common(1)[0][1]\n",
    "    # print(count_max_freq)\n",
    "\n",
    "    for token in total_vocab:\n",
    "        if token in processed_text[i]:\n",
    "           \n",
    "            # tf = 0.5+0.5*(counter[token]/max_count)\n",
    "            tf = 0.5+0.5*(counter[token]/count_max_freq)\n",
    "            # 0.5+0.5*(f(t,d)/ max(f(t`,d))\n",
    "           \n",
    "            tf_vec.append(tf)\n",
    "          \n",
    "           \n",
    "        else:\n",
    "            \n",
    "            tf = 0\n",
    "            tf_vec.append(tf)\n",
    "          \n",
    "\n",
    "    for j in range(len(total_vocab)):\n",
    "\n",
    "         df = doc_freq(total_vocab[j])\n",
    "         idf = np.log((N+1)/(df+1))\n",
    "\n",
    "         temp = tf_vec[j]*idf\n",
    "         tf_vec1.append(temp)\n",
    "   \n",
    "         TF_idf_dbl[i][j] = tf_vec[j]*idf\n",
    "    tf_idf1_dbl.append(tf_vec1)\n",
    "\n",
    "\n",
    " \n",
    "    # doc += 1\n",
    "\n",
    "np.save( \"TFidfdoublenormalisation\",TF_idf_dbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Matching Score\n",
      "\n",
      "Query: THE CRAB AND THE HERON\n",
      "\n",
      "['crab', 'heron']\n",
      "\n",
      "[122, 26, 270, 418, 169]\n",
      "8.901579233214175\n",
      "4.776156503341964\n",
      "2.2296098113058496\n",
      "2.210867165782778\n",
      "2.2099247671613873\n",
      "['crabhern.txt', 'aesop11.txt', 'long1-3.txt', 'timem.hac', 'fgoose.txt']\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02557373919250654, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3808980579800617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01753623169044474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02119659412756276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013919207191540366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.013919207191540366, 0.01753623169044474, 0.02119659412756276, 0.02557373919250654, 0.3808980579800617]\n",
      "0.3808980579800617  and index is  122\n",
      "0.02557373919250654  and index is  26\n",
      "0.02119659412756276  and index is  270\n",
      "0.01753623169044474  and index is  169\n",
      "0.013919207191540366  and index is  418\n",
      "\n",
      "List of Top Documents wrt Cosine Similarity for i/p query(Double Normalization) is:\n",
      "crabhern.txt  with Cosine Similarity Value: 0.3808980579800617 \n",
      " aesop11.txt  with Cosine Similarity Value: 0.02557373919250654 \n",
      " long1-3.txt  with Cosine Similarity Value: 0.02119659412756276 \n",
      " fgoose.txt  with Cosine Similarity Value: 0.01753623169044474 \n",
      " timem.hac  with Cosine Similarity Value: 0.013919207191540366 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def matching_score_5(k, query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "\n",
    "    print(\"Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    q_vec =[]\n",
    "    tfscore = 0\n",
    "    alltfscore =[]\n",
    "\n",
    "    for i in range(len(total_vocab)):\n",
    "        if total_vocab[i] in tokens:\n",
    "            Q[i] = 1\n",
    "        else:\n",
    "            Q[i] = 0\n",
    "    # print(Q)\n",
    "\n",
    "    for token in total_vocab:\n",
    "        if token in tokens:\n",
    "            q_vec.append(1)\n",
    "        else:\n",
    "            q_vec.append(0)\n",
    "    # print(q_vec)\n",
    "\n",
    "    for j in range(N):\n",
    "        tfscore = 0\n",
    "        for k in range(len(total_vocab)):\n",
    "            if(q_vec[k] == 1):\n",
    "                tfscore += TF_idf_dbl[j][k]\n",
    "            else:\n",
    "                tfscore = tfscore\n",
    "        alltfscore.append(tfscore)\n",
    "    # print(alltfscore)\n",
    "\n",
    "    alltfscore_sorted = sorted(alltfscore, reverse=True)\n",
    "    # print(alltfscore_sorted)\n",
    "\n",
    "    # print(sorted(range(len(alltfscore)), key=lambda i: alltfscore[i], reverse=True)[:5])\n",
    "    print(\"\")\n",
    "    docName = []\n",
    "\n",
    "    li=[]\n",
    "    for i in range(len(alltfscore)):\n",
    "      li.append([alltfscore[i],i])\n",
    "    li.sort( reverse=True)\n",
    "    sort_index = []\n",
    "    \n",
    "    for x in li:\n",
    "        sort_index.append(x[1])\n",
    "    \n",
    "    print((sort_index)[:5])\n",
    "    for index in((sort_index)[:5]):\n",
    "        head, tail = os.path.split(dataset[index][0])\n",
    "        docName.append(tail)\n",
    "        print(alltfscore[index])\n",
    "    print(docName)\n",
    "    # 11, 129, 224, 236, 366\n",
    "    # print(alltfscore[11], alltfscore[129], alltfscore[224], alltfscore[236], alltfscore[366])    \n",
    "\n",
    "    # Document Vectorization\n",
    "    cosine_score_dbl_nrml = []\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    D = np.zeros((N, total_vocab_size))\n",
    "    q_vec_1 = np.asarray(q_vec, dtype=np.float32)\n",
    "    for i in TF_idf_dbl:       \n",
    "        doc = i.reshape(1,total_vocab_size)\n",
    "        doc = np.array(doc)\n",
    "        # print(type(doc))\n",
    "        query = q_vec_1.reshape(1,total_vocab_size)\n",
    "        query = np.array(query)\n",
    "        cosSim = cosine_similarity(doc, query)\n",
    "        # list1 = cosSim[0].tolist()\n",
    "        list1 = cosSim[0].item()\n",
    "        cosine_score_dbl_nrml.append(list1)\n",
    "    print(cosine_score_dbl_nrml)\n",
    "\n",
    "    cosine_score_dbl_nrml_sorted = sorted(cosine_score_dbl_nrml)\n",
    "    docName = []\n",
    "    top5 = cosine_score_dbl_nrml_sorted[-5:]\n",
    "    print(top5)\n",
    "    for i in range(5):        \n",
    "        print(top5[4-i],\" and index is \",cosine_score_dbl_nrml.index(top5[4-i]))\n",
    "        index = cosine_score_dbl_nrml.index(top5[4-i])\n",
    "        head, tail = os.path.split(dataset[index][0])\n",
    "        tail.strip()\n",
    "        docName.append(tail.strip())\n",
    "        docName.append(\" with Cosine Similarity Value:\")\n",
    "        docName.append(top5[4-i])\n",
    "        docName.append(\"\\n\")\n",
    "    print(\"\\nList of Top Documents wrt Cosine Similarity for i/p query(Double Normalization) is:\")\n",
    "    print(*docName)  \n",
    "matching_score_5(5, \"THE CRAB AND THE HERON\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0aad90b67f09bf810afd339e05dcaf100a026c76413e1033464f3ebcb3a3614f4",
   "display_name": "Python 3.8.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}